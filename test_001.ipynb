{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'create_questions' from '/Users/anna/Dropbox/AOI/MoralLearning/CodeSets/value_learning_journal/value_learning_journal/create_questions.py'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# path to this codebase and data\n",
    "code_path = '/Users/anna/Dropbox/AOI/MoralLearning/CodeSets/value_learning_journal/value_learning_journal/'\n",
    "data_path = '/Users/anna/Dropbox/AOI/MoralLearning/CodeSets/value_learning_journal/data/test_001/'\n",
    "os.chdir(code_path)\n",
    "\n",
    "\n",
    "import main \n",
    "import create_questions\n",
    "import importlib\n",
    "importlib.reload(main)\n",
    "importlib.reload(create_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Prompts to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make dictionary of prompts with name and content\n",
    "# there are two prompting methods - one step gets all the items and their scores in one step, two step splits them.\n",
    "# prompt text should be organized accordingly\n",
    "\n",
    "# one step prompts\n",
    "p1 = f\"\"\"You are an expert in using what humans write and say to infer what they value and do not value. \\\n",
    "Below is a journal entry from the user. Among everything the user talks about, identify what items they consider most \\\n",
    "desirable and high value and give them a score from 0 - 100, where 100 is the highest value. Also identify any items they \\\n",
    "consider to be least desirable and low value, and give them a score from 0 - 100, where 0 denotes the lowest value. \\\n",
    "Items with high value are often virtuous, high utility, and bring meaning or fulfillment to them and others. \\\n",
    "Items with low value are immoral, low utility, and bring harm to themselves or others. \\\n",
    "Your task is to identify everything that the user identifies as having either a particularly high value or a particularly low value. \\\n",
    "Please identify at least 10 items. Return a list that includes all these items and their scores on a scale of 0 to 100, where 0 is the \\\n",
    "lowest value and 100 is the highest value. You must respond with a JSON packet with your response: \\\n",
    "1. values - list: \\\n",
    "1. item - <string>: the item \\\n",
    "2. score - <int>: that item's score\"\"\"\n",
    "\n",
    "p2 = f\"\"\"You are an expert in using what humans write and say to infer what they value and don't value. Given the user's journal entry, infer their values. Values are things a person finds virtuous and high utility, and \n",
    "which bring meaning or fulfillment to them or others. Return a json with each value and a score for how important it is to the user on a scale of 0 to 100, where 100 denotes the items with the highest value possible. \n",
    "1. values -  list: \\\n",
    "1. item - <string>: the item \\\n",
    "2. score - <int>: that item's score\"\"\"\n",
    "\n",
    "p0 = f\"\"\"You are an expert in using what humans write and say to infer what they value and don't value. Given the user's journal entry, infer their values.  Return a json with each value and a score for how important it is to the user on a scale of 0 to 100, where 100 denotes the items with the highest value possible.\\\n",
    "1. values -  list:\\\n",
    "1. item - <string>: the item\\\n",
    "2. score - <int>: that item's score\"\"\"\n",
    "\n",
    "prompt_dict_1 = {'p0': p0, 'p1': p1, 'p2': p2}\n",
    "\n",
    "# two-step prompts \n",
    "p3 = f\"\"\"You are an expert in using what humans write and say to infer what they value and don't value.\n",
    "What are the user's values according to this journal entry? Return a json called \"values\" with each value. 1. values -  list:\"\"\"\n",
    "\n",
    "p4 = f\"\"\"You are an expert in using what humans write and say to infer what they value and don't value. \\\n",
    "Given the user's journal entry, infer their values. Values are things a person finds virtuous and high utility, \\\n",
    "and which bring them meaning or fulfillment. Return a json with an entry called \"values\" with each value. \\\n",
    "1. values -  list:\"\"\"\n",
    "\n",
    "\n",
    "# applies as a second step to each of the above\n",
    "step2_prompt = f\"\"\"You are an expert in using what humans write and say to infer what they value and do not value.\\\n",
    "    Below is a journal entry from the user and a list of items. For each item, provide a score from 0-100, where 100 is the highest\\\n",
    "    value and 0 is the lowest value. Scores of high value mean that the item is virtuous, high utility, and which bring them meaning or fulfillment. Scores of low value mean the item is immoral, low utility, and bring harm to the actor or others.\\\n",
    "    You must respond with a JSON with an entry called values: 1. values - list:\\\n",
    "    1. value - <string>: the item. 2. score - <int>: that value's score\"\"\"\n",
    "\n",
    "\n",
    "prompt_dict_2 = {'p3': p3, 'p4': p4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test these prompts by running on selected journal entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'values': ['Productivity', 'Self-care', 'Achievement', 'Leisure', 'Supporting others']}\n",
      "{'values': ['Productivity', 'Self-care', 'Achievement', 'Leisure', 'Learning', 'Physical health', 'Supporting others', 'Introspection', 'Cleanliness']}\n"
     ]
    }
   ],
   "source": [
    "# generate for one entry -- to test\n",
    "journal_filename = data_path+'s00_text-e5.txt'\n",
    "main.test_one_step(journal_filename,'user', prompt_dict_1)\n",
    "main.test_two_step(journal_filename,'user', prompt_dict_2,step2_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s00_text-e1.txt\n",
      "{'values': ['Meaningful experiences', 'Gratitude', 'Validation', 'Supportive relationships', 'Engagement', 'Connection with others', 'Happiness', 'Appreciation of support', 'Satisfaction', 'Energy']}\n",
      "{'values': ['Meaningful experiences', 'Gratitude', 'Supportive relationships', 'Personal growth', 'Connection with others', 'Fulfillment through work', 'Engagement with like-minded individuals']}\n",
      "s00_text-e2.txt\n",
      "{'values': ['Friendship', 'Support', 'Meaningful connections', 'Wisdom', 'Confidence', 'Nature', 'Enjoyment']}\n",
      "{'values': ['Friendship', 'Support', 'Meaningful connections', 'Wisdom', 'Confidence', 'Nature', 'Resilience']}\n",
      "s00_text-e3.txt\n"
     ]
    }
   ],
   "source": [
    "# generate all\n",
    "\n",
    "journal_filename_list = ['s00_text-e1.txt','s00_text-e2.txt','s00_text-e3.txt','s00_text-e4.txt','s00_text-e5.txt']\n",
    "\n",
    "for journal_filename in journal_filename_list:\n",
    "\n",
    "    print(journal_filename)\n",
    "    main.test_one_step(data_path+journal_filename,'user', prompt_dict_1)\n",
    "    main.test_two_step(data_path+journal_filename,'user', prompt_dict_2,step2_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the output jsons you like to generate question items for this participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename_list = ['s00_text-e1_one_step_prompt_0.json','s00_text-e2_one_step_prompt_0.json','s00_text-e2_one_step_prompt_0.json','s00_text-e3_one_step_prompt_0.json','s00_text-e4_one_step_prompt_0.json','s00_text-e5_one_step_prompt_0.json']\n",
    "\n",
    "data_path = '/Users/anna/Dropbox/AOI/MoralLearning/CodeSets/value_learning_journal/data/'\n",
    "subjID='s00'\n",
    "create_questions.main(json_filename_list,data_path,subjID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
